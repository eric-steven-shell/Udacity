{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: the following is a reference document showing the test runs that led up to the choosing of the optimal parameters used in the classification algorithms.  It is included in the interest of reproducibility.  Although it is referred to a few times in the question and answer write up, it is not at all necessary to read all of the following in order to understand the logic of the decisions made in this project!</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Data and code set up and intialization</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, load_classifier_and_data, test_classifier\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import grid_search, svm\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, RFECV, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# all keys from data_dict\n",
    "features_list = ['poi',\n",
    " 'salary',\n",
    " 'bonus',\n",
    " 'deferral_payments',\n",
    " 'total_payments',\n",
    " 'exercised_stock_options',\n",
    " 'restricted_stock',\n",
    " 'shared_receipt_with_poi',\n",
    " 'restricted_stock_deferred',\n",
    " 'total_stock_value',\n",
    " 'expenses',\n",
    " 'loan_advances',\n",
    " 'to_messages',\n",
    " 'from_messages',\n",
    " 'other',\n",
    " 'from_this_person_to_poi',\n",
    " 'director_fees',\n",
    " 'deferred_income',\n",
    " 'long_term_incentive',\n",
    " 'from_poi_to_this_person',\n",
    " 'email_address']\n",
    "\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    " \n",
    "# put values of 0 where there are NaNs.\n",
    "# If there is no email address, update new feature has_email to 0, otherwise set it to 1.\n",
    "# Also, remove the actual email address, since it doesn't make sense to try to quantify \n",
    "# a text string for the email address (or do feature scaling, for instance)\n",
    "for emp, emp_dict in data_dict.items():    \n",
    "    has_email = 1\n",
    "    is_poi = 0\n",
    "    \n",
    "    for key, val in emp_dict.items():\n",
    "        if key == 'email_address' and val == 'NaN':\n",
    "            has_email = 0\n",
    "        elif key == 'poi' and val == True:\n",
    "            is_poi = 1\n",
    "        elif val == 'NaN':\n",
    "            emp_dict[key] = 0\n",
    "        \n",
    "    emp_dict['has_email'] = has_email\n",
    "    emp_dict['poi'] = is_poi\n",
    "    emp_dict.pop('email_address', 0)\n",
    "        \n",
    "data_dict.pop('TOTAL') # remove the invalid entry - no individual corresponds to 'TOTAL' entry in dictionary\n",
    "        \n",
    "features_list.remove('email_address')\n",
    "features_list.append('has_email')\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "# utility function to create stratified shuffle splits\n",
    "def sss(n_folds=100):\n",
    "    return StratifiedShuffleSplit(labels, n_iter=n_folds, random_state = 42)\n",
    "    \n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_dt = DecisionTreeClassifier(random_state=42, class_weight = 'balanced')\n",
    "clf_rf = RandomForestClassifier(random_state=42, max_depth=1, class_weight = 'balanced')\n",
    "clf_ab = AdaBoostClassifier(random_state=42)\n",
    "clf_ab_dt = AdaBoostClassifier(DecisionTreeClassifier(random_state=42, class_weight = 'balanced', max_depth=1), random_state=42)\n",
    "clf_lr = LogisticRegression(random_state=42, penalty ='l1', class_weight = 'balanced') # l1 to use for feature selection\n",
    "clf_sv = SVC(random_state=42, class_weight = 'balanced') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A few overview stats</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 145 individuals\n",
      "Number of POIs: 18\n",
      "Number of POIs with email: 18\n",
      "Number of features: 21\n"
     ]
    }
   ],
   "source": [
    "pois = [ indiv for indiv in my_dataset.keys() if my_dataset[indiv]['poi'] == 1]\n",
    "pois_with_email = [ indiv for indiv in my_dataset.keys() if my_dataset[indiv]['poi'] == 1 and \n",
    "                   my_dataset[indiv]['has_email'] == 1]\n",
    "num_features = len(data_dict['LAY KENNETH L'].keys())\n",
    "\n",
    "print(\"Dataset has {} individuals\".format(len(my_dataset)))\n",
    "print(\"Number of POIs: {}\".format(len(pois)))\n",
    "print(\"Number of POIs with email: {}\".format(len(pois_with_email)))\n",
    "print(\"Number of features: {}\".format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Feature selection and recursive feature elimination using L1 penalty with logistic regression </H2>\n",
    "\n",
    "As stated in http://scikit-learn.org/stable/modules/feature_selection.html, section 1.13.4.1. L1-based feature selection, linear models with L1 penalty can be used for feature selection, and this can be implemented with the SelectFromModel using a logistic regression model.  This was attempted below, in order to get a better feature selection effect than I got with SelectKBest and Principle Component Analysis (PCA), which actually seemed to worsen precision and recall scores.\n",
    "\n",
    "In addition, I used the logistic regression with L1 penalty for the recursive feature elimination, to get a reduced feature set initially with 7 features.  Since there were rankings, it was easy to add the next-most likely to help feature, and it was shown below that adding it helped in one classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "estimator = clf_lr # Use logistic regression with L1 penalty for feature selection\n",
    "\n",
    "selector_prec = RFECV(estimator, step=1, cv=sss(1000), scoring=\"precision\")\n",
    "selector_prec = selector_prec.fit(features, labels)\n",
    "selector_prec.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True,  True, False,  True,  True, False, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEXT, select some runs for the BL clfs above\n",
    "\n",
    "estimator = clf_lr\n",
    "\n",
    "selector_recall = RFECV(estimator, step=1, cv=sss(1000), scoring=\"recall\")\n",
    "selector_recall = selector_recall.fit(features, labels)\n",
    "selector_recall.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_f1 = RFECV(estimator, step=1, cv=sss(1000), scoring=\"f1\")\n",
    "selector_f1 = selector_f1.fit(features, labels)\n",
    "selector_f1.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False  True  True False False False  True\n",
      "  True False  True  True False False  True False]\n",
      "[ 5  8  3 10  9 12  1  1 13  2 11  1  1  6  1  1  4  7  1 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'director_fees',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_messages']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = selector_recall # use the recall RFECV for the tentative feature selection since it leaves 7 features, more than for precision and f1\n",
    "\n",
    "print selector.support_ \n",
    "print selector.ranking_\n",
    "\n",
    "lr_support = selector.support_\n",
    "lr_ranking = selector.ranking_\n",
    "\n",
    "lr_features_list = list(np.array(features_list)[1:][selector.support_])\n",
    "lr_features_list.sort()\n",
    "lr_features_list = [\"poi\"] + lr_features_list\n",
    "lr_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  8  3 10  9 12  1  1 13  2 11  1  1  6  1  1  4  7  1 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'director_fees',\n",
       " 'from_messages',\n",
       " 'from_poi_to_this_person',\n",
       " 'from_this_person_to_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_messages']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_features_list = list(np.array(features_list)[1:][selector_recall.support_])\n",
    "lr_features_list.sort()\n",
    "lr_features_list = [\"poi\"] + lr_features_list\n",
    "print(selector_recall.ranking_)\n",
    "lr_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Using the rankings, we can get the list <i>below</i> of most valuable features for the L1 penalty based feature selection</H4>  All features not eliminated by the recursive feature elimination have rank 1, and POI feature arbitrarily assigned rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 0, feature: poi\n",
      "rank: 5, feature: salary\n",
      "rank: 8, feature: bonus\n",
      "rank: 3, feature: deferral_payments\n",
      "rank: 10, feature: total_payments\n",
      "rank: 9, feature: exercised_stock_options\n",
      "rank: 12, feature: restricted_stock\n",
      "rank: 1, feature: shared_receipt_with_poi\n",
      "rank: 1, feature: restricted_stock_deferred\n",
      "rank: 13, feature: total_stock_value\n",
      "rank: 2, feature: expenses\n",
      "rank: 11, feature: loan_advances\n",
      "rank: 1, feature: to_messages\n",
      "rank: 1, feature: from_messages\n",
      "rank: 6, feature: other\n",
      "rank: 1, feature: from_this_person_to_poi\n",
      "rank: 1, feature: director_fees\n",
      "rank: 4, feature: deferred_income\n",
      "rank: 7, feature: long_term_incentive\n",
      "rank: 1, feature: from_poi_to_this_person\n",
      "rank: 14, feature: has_email\n"
     ]
    }
   ],
   "source": [
    "rank = list(selector_recall.ranking_)\n",
    "rank.insert(0, 0) # put a rank of 0 in for the feature importance rank of poi, for display\n",
    "for ind, feat in zip(rank, features_list):\n",
    "    print(\"rank: {}, feature: {}\".format(ind,feat))\n",
    "    \n",
    "# NOTE that ranks printed for all features selected by the RECV will have rank = 1; rank of poi arbritrarily listed as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Algorithm tuning and validation</H2>\n",
    "<br>\n",
    "<i>Below we see multiple runs where we use the grid search with cross validation to hone in on the optimal parameter values for Random Forest and Adaptive Boosting algorithms.  These, used in conjuction with the F1-based feature selection in the pipeline, eventually yielded values for both precision and recall above 0.3.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.55, n_estimators=33, random_state=42)\n",
      "\tAccuracy: 0.84691\tPrecision: 0.21405\tRecall: 0.25600\tF1: 0.23315\tF2: 0.24634\n",
      "\tTotal predictions: 5500\tTrue positives:  128\tFalse positives:  470\tFalse negatives:  372\tTrue negatives: 4530\n",
      "\n",
      "done in 30.037s\n"
     ]
    }
   ],
   "source": [
    "clf_ab5 = AdaBoostClassifier(random_state=42, n_estimators=33, learning_rate=0.55)\n",
    "t0 = time()\n",
    "test_classifier(clf_ab5, my_dataset, lr_features_list, folds = 500)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.55, n_estimators=33, random_state=42)\n",
      "\tAccuracy: 0.84707\tPrecision: 0.40108\tRecall: 0.29800\tF1: 0.34194\tF2: 0.31415\n",
      "\tTotal predictions: 7500\tTrue positives:  298\tFalse positives:  445\tFalse negatives:  702\tTrue negatives: 6055\n",
      "\n",
      "done in 33.222s\n"
     ]
    }
   ],
   "source": [
    "clf_ab5 = AdaBoostClassifier(random_state=42, n_estimators=33, learning_rate=0.55)\n",
    "t0 = time()\n",
    "test_classifier(clf_ab5, my_dataset, features_list, folds = 500)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.3, n_estimators=15, random_state=42)\n",
      "\tAccuracy: 0.88485\tPrecision: 0.35401\tRecall: 0.32333\tF1: 0.33798\tF2: 0.32904\n",
      "\tTotal predictions: 3300\tTrue positives:   97\tFalse positives:  177\tFalse negatives:  203\tTrue negatives: 2823\n",
      "\n",
      "done in 9.656s\n"
     ]
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
    "            max_depth=3, random_state=42)\n",
    "cl_ab6 = AdaBoostClassifier(dt2, random_state=42, n_estimators=15, learning_rate=0.3)\n",
    "\n",
    "t0 = time()\n",
    "test_classifier(cl_ab6, my_dataset, lr_features_list, folds = 300)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.3, n_estimators=15, random_state=42)\n",
      "\tAccuracy: 0.82310\tPrecision: 0.36008\tRecall: 0.30667\tF1: 0.33123\tF2: 0.31604\n",
      "\tTotal predictions: 4200\tTrue positives:  184\tFalse positives:  327\tFalse negatives:  416\tTrue negatives: 3273\n",
      "\n",
      "done in 9.302s\n"
     ]
    }
   ],
   "source": [
    "lr_features_list.append('expenses')  # append the next feature that would have been selected by RFECV, with rank = 2\n",
    "t0 = time()\n",
    "test_classifier(cl_ab6, my_dataset, lr_features_list, folds = 300)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.3, n_estimators=15, random_state=42)\n",
      "\tAccuracy: 0.83452\tPrecision: 0.39560\tRecall: 0.30000\tF1: 0.34123\tF2: 0.31524\n",
      "\tTotal predictions: 4200\tTrue positives:  180\tFalse positives:  275\tFalse negatives:  420\tTrue negatives: 3325\n",
      "\n",
      "done in 9.361s\n"
     ]
    }
   ],
   "source": [
    "lr_features_list.append('deferral_payments')  # append the next feature fron selected by RFECV, with rank = 3 and rerun\n",
    "t0 = time()\n",
    "test_classifier(cl_ab6, my_dataset, lr_features_list, folds = 300)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in the last feature caused recall to drop from 0.33667 to 0.31833, so we'll remove this feature and stop here\n",
    "lr_features_list.remove('deferral_payments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 52.370s\n",
      "Pipeline(memory=None,\n",
      "     steps=[('ADA', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=12, random_state=42))])\n",
      "\tAccuracy: 0.85067\tPrecision: 0.41549\tRecall: 0.29500\tF1: 0.34503\tF2: 0.31316\n",
      "\tTotal predictions: 1500\tTrue positives:   59\tFalse positives:   83\tFalse negatives:  141\tTrue negatives: 1217\n",
      "\n",
      "done in 2.497s\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(labels, 300, random_state=42)\n",
    "pipe = Pipeline(steps=[(\"ADA\", clf_ab)])\n",
    "parameters_ada = {\n",
    "                   'ADA__n_estimators':[1,4,8,12],\n",
    "                'ADA__learning_rate':[0.1,0.5,1]}\n",
    "                \n",
    "gs = GridSearchCV(pipe, parameters_ada, scoring=\"f1\", cv = sss)\n",
    "t0 = time()\n",
    "gs.fit(features, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "t0 = time()\n",
    "test_classifier(gs.best_estimator_, my_dataset, features_list, folds = 100)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 2, 'clf__n_estimators': 120}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...timators=120, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80067\tPrecision: 0.34185\tRecall: 0.53500\tF1: 0.41715\tF2: 0.48068\n",
      "\tTotal predictions: 4500\tTrue positives:  321\tFalse positives:  618\tFalse negatives:  279\tTrue negatives: 3282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf1 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', clf_rf1)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth': [2, 3, 4, 5],\n",
    "    'clf__n_estimators': [40, 60, 90, 120]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"f1\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 2, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.78467\tPrecision: 0.32070\tRecall: 0.55000\tF1: 0.40516\tF2: 0.48119\n",
      "\tTotal predictions: 4500\tTrue positives:  330\tFalse positives:  699\tFalse negatives:  270\tTrue negatives: 3201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf1 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),('select', select),('clf', clf_rf1)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth': [2, 3, 4, 5],\n",
    "    'clf__n_estimators': [40, 60, 90, 120]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"f1\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 3, 'select__estimator__tol': 0.01, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80556\tPrecision: 0.32573\tRecall: 0.42833\tF1: 0.37005\tF2: 0.40295\n",
      "\tTotal predictions: 4500\tTrue positives:  257\tFalse positives:  532\tFalse negatives:  343\tTrue negatives: 3368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 3, 'select__estimator__tol': 0.01, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80556\tPrecision: 0.32573\tRecall: 0.42833\tF1: 0.37005\tF2: 0.40295\n",
      "\tTotal predictions: 4500\tTrue positives:  257\tFalse positives:  532\tFalse negatives:  343\tTrue negatives: 3368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 90, 120]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80127\tPrecision: 0.31987\tRecall: 0.43550\tF1: 0.36883\tF2: 0.40614\n",
      "\tTotal predictions: 15000\tTrue positives:  871\tFalse positives: 1852\tFalse negatives: 1129\tTrue negatives: 11148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[0. 0. ... 1. 0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__max_depth': [2, 3, 4], 'select__estimator__tol': [0.1, 0.01, 0.001, 0.0001, 1e-05], 'clf__n_estimators': [40, 60, 90, 120]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "  ('select',\n",
       "   SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
       "             solver='liblinear', tol=0.01, verbose=0, warm_start=False),\n",
       "           norm_order=1, prefit=False, threshold=None)),\n",
       "  ('clf', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "               criterion='gini', max_depth=3, max_features='auto',\n",
       "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "               min_impurity_split=None, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
       "               verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner1 = grid_search.best_estimator_\n",
    "winner1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80127\tPrecision: 0.31987\tRecall: 0.43550\tF1: 0.36883\tF2: 0.40614\n",
      "\tTotal predictions: 15000\tTrue positives:  871\tFalse positives: 1852\tFalse negatives: 1129\tTrue negatives: 11148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(winner1, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.77071\tPrecision: 0.29945\tRecall: 0.45167\tF1: 0.36013\tF2: 0.40998\n",
      "\tTotal predictions: 4200\tTrue positives:  271\tFalse positives:  634\tFalse negatives:  329\tTrue negatives: 2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(winner1, my_dataset, lr_features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.76936\tPrecision: 0.30298\tRecall: 0.47250\tF1: 0.36921\tF2: 0.42495\n",
      "\tTotal predictions: 14000\tTrue positives:  945\tFalse positives: 2174\tFalse negatives: 1055\tTrue negatives: 9826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(winner1, my_dataset, lr_features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'from_poi_to_this_person']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_without_mine = features_list\n",
    "features_without_mine.remove('has_email')\n",
    "features_without_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.81867\tPrecision: 0.35165\tRecall: 0.42667\tF1: 0.38554\tF2: 0.40921\n",
      "\tTotal predictions: 4500\tTrue positives:  256\tFalse positives:  472\tFalse negatives:  344\tTrue negatives: 3428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rerun test against features minus the feature added\n",
    "#  compare to baseline with 300 folds: Accuracy: 0.83044\tPrecision: 0.37519\tRecall: 0.40833\tF1: 0.39106\tF2: 0.40124\n",
    "test_classifier(winner1, my_dataset, features_without_mine, folds = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the feature added, has_email, has no benefit, at least with the best so far classifier and 300 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'select__estimator__tol': 0.01}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.01, verbose=0, warm_start=False),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.28800\tPrecision: 0.14290\tRecall: 0.86833\tF1: 0.24541\tF2: 0.43086\n",
      "\tTotal predictions: 4500\tTrue positives:  521\tFalse positives: 3125\tFalse negatives:   79\tTrue negatives:  775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()), ('select', select), ('clf', GaussianNB())])\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'select__estimator__C': [0.1, 0.5, 0.9, 1.0]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.81735475e-01, 5.21875000e-01, 4.55198951e-01, 4.33029255e-02,\n",
       "       5.03529074e-02, 1.57231836e-01, 2.54845137e-01, 9.63456735e-02,\n",
       "       3.60830823e-02, 6.06216914e-02, 0.00000000e+00, 1.91563800e-01,\n",
       "       1.52770045e-01, 1.46721985e-05, 1.06732348e-01, 0.00000000e+00,\n",
       "       1.20800334e-01, 5.92379574e-02, 8.90151515e-02, 1.00000000e+00])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.019550e+05,  4.175000e+06,  2.869717e+06,  4.484442e+06,\n",
       "        1.729541e+06,  1.260270e+05,  1.407000e+03, -1.260270e+05,\n",
       "        1.729541e+06,  1.386800e+04,  0.000000e+00,  2.902000e+03,\n",
       "        2.195000e+03,  1.520000e+02,  6.500000e+01,  0.000000e+00,\n",
       "       -3.081055e+06,  3.048050e+05,  4.700000e+01,  1.000000e+00])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 3, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.81511\tPrecision: 0.35204\tRecall: 0.46000\tF1: 0.39884\tF2: 0.43342\n",
      "\tTotal predictions: 4500\tTrue positives:  276\tFalse positives:  508\tFalse negatives:  324\tTrue negatives: 3392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf1 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', clf_rf1)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 90]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features_scaled, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.79167\tPrecision: 0.33087\tRecall: 0.44833\tF1: 0.38075\tF2: 0.41861\n",
      "\tTotal predictions: 4200\tTrue positives:  269\tFalse positives:  544\tFalse negatives:  331\tTrue negatives: 3056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(grid_search.best_estimator_, my_dataset, lr_features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 3, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.81511\tPrecision: 0.35204\tRecall: 0.46000\tF1: 0.39884\tF2: 0.43342\n",
      "\tTotal predictions: 4500\tTrue positives:  276\tFalse positives:  508\tFalse negatives:  324\tTrue negatives: 3392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf1 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', clf_rf1)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 90]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features_scaled, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 2, 'clf__n_estimators': 90}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=90, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80200\tPrecision: 0.34338\tRecall: 0.53167\tF1: 0.41727\tF2: 0.47912\n",
      "\tTotal predictions: 4500\tTrue positives:  319\tFalse positives:  610\tFalse negatives:  281\tTrue negatives: 3290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.5, 'select__estimator__tol': 0.01, 'clf__n_estimators': 90}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.01, verbose=0, warm_start=False),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=90, random_state=42))])\n",
      "\tAccuracy: 0.85822\tPrecision: 0.46091\tRecall: 0.37333\tF1: 0.41252\tF2: 0.38808\n",
      "\tTotal predictions: 4500\tTrue positives:  224\tFalse positives:  262\tFalse negatives:  376\tTrue negatives: 3638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001],\n",
    "    'clf__learning_rate': [ 0.1, 0.5, 1.0],\n",
    "    'clf__n_estimators': [ 10, 30, 50, 90, 140]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('ADA', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=12, random_state=42))])\n",
      "\tAccuracy: 0.84429\tPrecision: 0.43891\tRecall: 0.32333\tF1: 0.37236\tF2: 0.34131\n",
      "\tTotal predictions: 4200\tTrue positives:  194\tFalse positives:  248\tFalse negatives:  406\tTrue negatives: 3352\n",
      "\n",
      "done in 4209.499s\n"
     ]
    }
   ],
   "source": [
    "test_classifier(gs.best_estimator_, my_dataset, lr_features_list, folds = 300)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'bonus',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'to_messages',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'from_poi_to_this_person']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 2, 'select__estimator__tol': 0.0001, 'clf__n_estimators': 80}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80111\tPrecision: 0.34292\tRecall: 0.53667\tF1: 0.41845\tF2: 0.48218\n",
      "\tTotal predictions: 4500\tTrue positives:  322\tFalse positives:  617\tFalse negatives:  278\tTrue negatives: 3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf2 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', clf_rf2)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "grid_search.fit(features, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_grid1 = grid_search\n",
    "winner_clf1 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_classifier_and_data(winner_clf1, my_dataset, features_list)\n",
    "\n",
    "# dump_classifier_and_data(clf, dataset, feature_list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.79467\tPrecision: 0.33466\tRecall: 0.54650\tF1: 0.41512\tF2: 0.48509\n",
      "\tTotal predictions: 15000\tTrue positives: 1093\tFalse positives: 2173\tFalse negatives:  907\tTrue negatives: 10827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import dump_classifier_and_data, load_classifier_and_data, test_classifier\n",
    "\n",
    "pkl_clf, pkl_dataset, pkl_feature_list = load_classifier_and_data()\n",
    "test_classifier(pkl_clf, pkl_dataset, pkl_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(clf):  # assuming that features, labels initiated\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    clf.fit(features_train, labels_train)\n",
    "    labels_predicted = clf.predict(features_test)\n",
    "    classification_rep = classification_report(labels_test, labels_predicted)\n",
    "    print(\"classification_report for {}: \\n{}\".format(clf, classification_rep))\n",
    "    test_classifier(clf, my_dataset, features_list, folds = 150)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ab_dt = AdaBoostClassifier(DecisionTreeClassifier(random_state=42, class_weight = 'balanced', max_depth=1), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Quick tests using simple test/train splits and classification report</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report for GaussianNB(priors=None): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.95      0.94        39\n",
      "        1.0       0.50      0.40      0.44         5\n",
      "\n",
      "avg / total       0.88      0.89      0.88        44\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.73067\tPrecision: 0.23529\tRecall: 0.45333\tF1: 0.30979\tF2: 0.38245\n",
      "\tTotal predictions: 2250\tTrue positives:  136\tFalse positives:  442\tFalse negatives:  164\tTrue negatives: 1508\n",
      "\n",
      "classification_report for DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.95      0.92        39\n",
      "        1.0       0.33      0.20      0.25         5\n",
      "\n",
      "avg / total       0.84      0.86      0.85        44\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.79156\tPrecision: 0.20761\tRecall: 0.20000\tF1: 0.20374\tF2: 0.20148\n",
      "\tTotal predictions: 2250\tTrue positives:   60\tFalse positives:  229\tFalse negatives:  240\tTrue negatives: 1721\n",
      "\n",
      "classification_report for RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=1, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.72      0.81        39\n",
      "        1.0       0.21      0.60      0.32         5\n",
      "\n",
      "avg / total       0.85      0.70      0.76        44\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=1, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.77289\tPrecision: 0.31059\tRecall: 0.57667\tF1: 0.40373\tF2: 0.49232\n",
      "\tTotal predictions: 2250\tTrue positives:  173\tFalse positives:  384\tFalse negatives:  127\tTrue negatives: 1566\n",
      "\n",
      "classification_report for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.92      0.91        39\n",
      "        1.0       0.25      0.20      0.22         5\n",
      "\n",
      "avg / total       0.83      0.84      0.83        44\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42)\n",
      "\tAccuracy: 0.84756\tPrecision: 0.40693\tRecall: 0.31333\tF1: 0.35405\tF2: 0.32844\n",
      "\tTotal predictions: 2250\tTrue positives:   94\tFalse positives:  137\tFalse negatives:  206\tTrue negatives: 1813\n",
      "\n",
      "classification_report for AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.90      0.90        39\n",
      "        1.0       0.20      0.20      0.20         5\n",
      "\n",
      "avg / total       0.82      0.82      0.82        44\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42)\n",
      "\tAccuracy: 0.83867\tPrecision: 0.38869\tRecall: 0.36667\tF1: 0.37736\tF2: 0.37087\n",
      "\tTotal predictions: 2250\tTrue positives:  110\tFalse positives:  173\tFalse negatives:  190\tTrue negatives: 1777\n",
      "\n",
      "classification_report for LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.85      0.89        39\n",
      "        1.0       0.33      0.60      0.43         5\n",
      "\n",
      "avg / total       0.87      0.82      0.84        44\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.82533\tPrecision: 0.39913\tRecall: 0.61333\tF1: 0.48357\tF2: 0.55388\n",
      "\tTotal predictions: 2250\tTrue positives:  184\tFalse positives:  277\tFalse negatives:  116\tTrue negatives: 1673\n",
      "\n",
      "classification_report for SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False): \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94        39\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.79      0.89      0.83        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "quick_test(clf_nb)\n",
    "quick_test(clf_dt)\n",
    "quick_test(clf_rf)\n",
    "quick_test(clf_ab)\n",
    "quick_test(clf_ab_dt)\n",
    "quick_test(clf_lr)\n",
    "quick_test(clf_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>... and on to more systematic use of GridSearchCV to find optimal parameters for the algorithms and the L1-based \n",
    "feature selection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 3, 'select__estimator__tol': 0.01, 'clf__n_estimators': 40}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...stimators=40, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.81867\tPrecision: 0.35165\tRecall: 0.42667\tF1: 0.38554\tF2: 0.40921\n",
      "\tTotal predictions: 4500\tTrue positives:  256\tFalse positives:  472\tFalse negatives:  344\tTrue negatives: 3428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf2 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),('select', select),('clf', clf_rf2)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "scaler_grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "scaler_grid_search.fit(features, labels)\n",
    "print(scaler_grid_search.best_params_)\n",
    "\n",
    "test_classifier(scaler_grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 2, 'select__estimator__tol': 0.0001, 'clf__n_estimators': 80}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.80111\tPrecision: 0.34292\tRecall: 0.53667\tF1: 0.41845\tF2: 0.48218\n",
      "\tTotal predictions: 4500\tTrue positives:  322\tFalse positives:  617\tFalse negatives:  278\tTrue negatives: 3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf2 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('select', select),('clf', clf_rf2)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "prescaled_grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "prescaled_grid_search.fit(features, labels)\n",
    "print(prescaled_grid_search.best_params_)\n",
    "\n",
    "test_classifier(prescaled_grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 7, 'clf__max_depth': 2, 'select__k': 15, 'clf__n_estimators': 80}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectKBest(k=15, score_func=<function f_classif at 0x000000000BC02208>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestCla...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.73667\tPrecision: 0.27517\tRecall: 0.59667\tF1: 0.37664\tF2: 0.48365\n",
      "\tTotal predictions: 4500\tTrue positives:  358\tFalse positives:  943\tFalse negatives:  242\tTrue negatives: 2957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rf2 = RandomForestClassifier(criterion='gini', random_state=42, class_weight='balanced')\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()), \n",
    "                     ('select', SelectKBest()), \n",
    "                     ('pca', PCA(random_state=42)), \n",
    "                     ('clf', clf_rf2)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': [7,10,15],\n",
    "    'pca__n_components': [3,5,7],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "selectk_pca_grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "selectk_pca_grid_search.fit(features, labels)\n",
    "print(selectk_pca_grid_search.best_params_)\n",
    "\n",
    "test_classifier(selectk_pca_grid_search.best_estimator_, my_dataset, features_list, folds = 300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "select = SelectFromModel(LogisticRegression(class_weight='balanced', max_iter=100, penalty='l1', random_state=42))\n",
    "sss = StratifiedShuffleSplit(labels, 100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.5, 'select__estimator__tol': 0.01, 'clf__n_estimators': 80}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.01, verbose=0, warm_start=False),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=80, random_state=42))])\n",
      "\tAccuracy: 0.86156\tPrecision: 0.47569\tRecall: 0.37500\tF1: 0.41938\tF2: 0.39158\n",
      "\tTotal predictions: 4500\tTrue positives:  225\tFalse positives:  248\tFalse negatives:  375\tTrue negatives: 3652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ab2 = AdaBoostClassifier(random_state=42)\n",
    "pipeline = Pipeline([('select', select),('clf', clf_ab2)])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__learning_rate': [0.1, 0.5, 1.0],\n",
    "    'clf__n_estimators': [20, 40, 60, 80, 100]\n",
    "}\n",
    "              \n",
    "adab_grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "adab_grid_search.fit(features, labels)\n",
    "print(adab_grid_search.best_params_)\n",
    "\n",
    "test_classifier(adab_grid_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.01, verbose=0, warm_start=False),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=80, random_state=42))])\n",
      "\tAccuracy: 0.85667\tPrecision: 0.45253\tRecall: 0.35750\tF1: 0.39944\tF2: 0.37317\n",
      "\tTotal predictions: 15000\tTrue positives:  715\tFalse positives:  865\tFalse negatives: 1285\tTrue negatives: 12135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(adab_grid_search.best_estimator_, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__base_estimator__max_depth': 1, 'clf__learning_rate': 0.1, 'select__estimator__tol': 0.0001, 'clf__n_estimators': 20}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...e=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=20, random_state=42))])\n",
      "\tAccuracy: 0.71089\tPrecision: 0.24171\tRecall: 0.54667\tF1: 0.33521\tF2: 0.43652\n",
      "\tTotal predictions: 4500\tTrue positives:  328\tFalse positives: 1029\tFalse negatives:  272\tTrue negatives: 2871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ab3 = AdaBoostClassifier(DecisionTreeClassifier(random_state=42, class_weight = 'balanced'), random_state=42)\n",
    "pipeline = Pipeline([('select', select),('clf', clf_ab3)])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.01, 0.001, 0.0001, 0.00001],\n",
    "    'clf__learning_rate': [0.1, 0.5, 1.0],\n",
    "    'clf__n_estimators': [20, 40, 60, 80, 100],\n",
    "    'clf__base_estimator__max_depth': [1,2,3,4]\n",
    "}\n",
    "\n",
    "adab_grid_search2 = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "adab_grid_search2.fit(features, labels)\n",
    "print(adab_grid_search2.best_params_)\n",
    "\n",
    "test_classifier(adab_grid_search2.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 5, 'clf__max_depth': 2, 'select__k': 6, 'clf__n_estimators': 80}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectKBest(k=6, score_func=<function f_classif at 0x000000000BC02208>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "           ...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.79067\tPrecision: 0.31042\tRecall: 0.46667\tF1: 0.37284\tF2: 0.42399\n",
      "\tTotal predictions: 4500\tTrue positives:  280\tFalse positives:  622\tFalse negatives:  320\tTrue negatives: 3278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('select', SelectKBest()), \n",
    "                     ('pca', PCA(random_state=42)), \n",
    "                     ('clf', clf_rf2)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': [6,8,12],\n",
    "    'pca__n_components': [3,4,5,6],\n",
    "    'clf__max_depth': [2, 3, 4],\n",
    "    'clf__n_estimators': [40, 60, 80]\n",
    "}\n",
    "\n",
    "features_scaled = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "prescaled_grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"precision\")\n",
    "prescaled_grid_search.fit(features_scaled, labels)\n",
    "print(prescaled_grid_search.best_params_)\n",
    "\n",
    "test_classifier(prescaled_grid_search.best_estimator_, my_dataset, features_list, folds = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [10] are constant.\n",
      "  UserWarning)\n",
      "C:\\anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectKBest(k=6, score_func=<function f_classif at 0x000000000BC02208>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "           ...stimators=80, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.78660\tPrecision: 0.30305\tRecall: 0.46200\tF1: 0.36601\tF2: 0.41814\n",
      "\tTotal predictions: 15000\tTrue positives:  924\tFalse positives: 2125\tFalse negatives: 1076\tTrue negatives: 10875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(prescaled_grid_search.best_estimator_, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.45, 'select__estimator__tol': 0.0005, 'clf__n_estimators': 20}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0005, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=20, random_state=42))])\n",
      "\tAccuracy: 0.87533\tPrecision: 0.54503\tRecall: 0.39333\tF1: 0.45692\tF2: 0.41652\n",
      "\tTotal predictions: 4500\tTrue positives:  236\tFalse positives:  197\tFalse negatives:  364\tTrue negatives: 3703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ab2 = AdaBoostClassifier(random_state=42)\n",
    "pipeline = Pipeline([('select', select),('clf', clf_ab2)])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.0005, 0.001, 0.005],\n",
    "    'clf__learning_rate': [0.45, 0.5, 0.55],\n",
    "    'clf__n_estimators': [10, 15, 20, 25, 30]\n",
    "}\n",
    "              \n",
    "adab_grid_search2 = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "adab_grid_search2.fit(features, labels)\n",
    "print(adab_grid_search2.best_params_)\n",
    "\n",
    "test_classifier(adab_grid_search2.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0005, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=20, random_state=42))])\n",
      "\tAccuracy: 0.87307\tPrecision: 0.53297\tRecall: 0.38800\tF1: 0.44907\tF2: 0.41032\n",
      "\tTotal predictions: 15000\tTrue positives:  776\tFalse positives:  680\tFalse negatives: 1224\tTrue negatives: 12320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(adab_grid_search2.best_estimator_, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('select',\n",
       "   SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
       "             solver='liblinear', tol=0.0005, verbose=0, warm_start=False),\n",
       "           norm_order=1, prefit=False, threshold=None)),\n",
       "  ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=0.45, n_estimators=20, random_state=42))]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adab_grid_search2.best_estimator_.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'select__k': 12, 'pca__n_components': 3, 'clf__learning_rate': 1.0, 'clf__n_estimators': 20}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectKBest(k=12, score_func=<function f_classif at 0x000000000BC02208>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=20, random_state=42))])\n",
      "\tAccuracy: 0.81889\tPrecision: 0.22078\tRecall: 0.14167\tF1: 0.17259\tF2: 0.15260\n",
      "\tTotal predictions: 4500\tTrue positives:   85\tFalse positives:  300\tFalse negatives:  515\tTrue negatives: 3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('select', SelectKBest()),\n",
    "                     ('pca', PCA(random_state=42)), \n",
    "                     ('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': [6,8,12],\n",
    "    'pca__n_components': [3,5,6],\n",
    "    'clf__learning_rate': [0.1, 0.5, 1.0],\n",
    "    'clf__n_estimators': [20, 30, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "grid_search.fit(features_scaled, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.5, 'clf__n_estimators': 50}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=50, random_state=42))])\n",
      "\tAccuracy: 0.84622\tPrecision: 0.40043\tRecall: 0.30833\tF1: 0.34840\tF2: 0.32320\n",
      "\tTotal predictions: 4500\tTrue positives:  185\tFalse positives:  277\tFalse negatives:  415\tTrue negatives: 3623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'clf__learning_rate': [0.1, 0.5, 1.0],\n",
    "    'clf__n_estimators': [20, 30, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "grid_search.fit(features_scaled, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0005, verbose=0, warm_...thm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=20, random_state=42))])\n",
      "\tAccuracy: 0.87280\tPrecision: 0.53181\tRecall: 0.38450\tF1: 0.44631\tF2: 0.40705\n",
      "\tTotal predictions: 15000\tTrue positives:  769\tFalse positives:  677\tFalse negatives: 1231\tTrue negatives: 12323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_final = AdaBoostClassifier(learning_rate=0.5, n_estimators=20, random_state=42)\n",
    "selection_final = SelectFromModel(estimator=\n",
    "                                  LogisticRegression(class_weight='balanced', penalty='l1', random_state=42, tol=0.0005))\n",
    "pipeline_final = Pipeline([('select', selection_final), ('clf', clf_final)])\n",
    "\n",
    "dump_classifier_and_data(pipeline_final, my_dataset, features_list)\n",
    "pkl_clf, pkl_dataset, pkl_feature_list = load_classifier_and_data()\n",
    "test_classifier(pkl_clf, pkl_dataset, pkl_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=20, random_state=42)\n",
      "\tAccuracy: 0.85881\tPrecision: 0.50801\tRecall: 0.37000\tF1: 0.42816\tF2: 0.39126\n",
      "\tTotal predictions: 4200\tTrue positives:  222\tFalse positives:  215\tFalse negatives:  378\tTrue negatives: 3385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to testing with reduced # features from RFECV w/ logistic regr\n",
    "\n",
    "test_classifier(clf_final, my_dataset, lr_features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Looks like we have a winner here.  Best to test if there are improvements by using other types of feature selection, as well as different features (the features obtained from recursive feature elimination, and the features minus the feature added for having an email).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.45, 'select__estimator__tol': 0.0001, 'clf__n_estimators': 18}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))])\n",
      "\tAccuracy: 0.87422\tPrecision: 0.53881\tRecall: 0.39333\tF1: 0.45472\tF2: 0.41579\n",
      "\tTotal predictions: 4500\tTrue positives:  236\tFalse positives:  202\tFalse negatives:  364\tTrue negatives: 3698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('select', select),('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.0001, 0.0005, 0.001],\n",
    "    'clf__learning_rate': [0.40, 0.45, 0.5],\n",
    "    'clf__n_estimators': [15, 18, 20, 22, 25]\n",
    "}\n",
    " \n",
    "# same pipeline and classifier as best so far, with slightly different param choices\n",
    "adab_grid_search3 = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "adab_grid_search3.fit(features, labels)\n",
    "print(adab_grid_search3.best_params_)\n",
    "\n",
    "test_classifier(adab_grid_search3.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))])\n",
      "\tAccuracy: 0.87200\tPrecision: 0.52736\tRecall: 0.38550\tF1: 0.44541\tF2: 0.40742\n",
      "\tTotal predictions: 15000\tTrue positives:  771\tFalse positives:  691\tFalse negatives: 1229\tTrue negatives: 12309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(adab_grid_search3.best_estimator_, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))])\n",
      "\tAccuracy: 0.85929\tPrecision: 0.51016\tRecall: 0.37667\tF1: 0.43337\tF2: 0.39747\n",
      "\tTotal predictions: 4200\tTrue positives:  226\tFalse positives:  217\tFalse negatives:  374\tTrue negatives: 3383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(adab_grid_search3.best_estimator_, my_dataset, lr_features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))])\n",
      "\tAccuracy: 0.87422\tPrecision: 0.53881\tRecall: 0.39333\tF1: 0.45472\tF2: 0.41579\n",
      "\tTotal predictions: 4500\tTrue positives:  236\tFalse positives:  202\tFalse negatives:  364\tTrue negatives: 3698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(adab_grid_search3.best_estimator_, my_dataset, features_without_mine, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'select__estimator__tol': 0.0001}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,...'SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))]))])\n",
      "\tAccuracy: 0.87644\tPrecision: 0.56111\tRecall: 0.33667\tF1: 0.42083\tF2: 0.36594\n",
      "\tTotal predictions: 4500\tTrue positives:  202\tFalse positives:  158\tFalse negatives:  398\tTrue negatives: 3742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Does using feature scaling before the select-from-model using L1 penalty help?\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('select', select),\n",
    "                     ('clf', adab_grid_search3.best_estimator_)])\n",
    "\n",
    "param_grid = {\n",
    "    'select__estimator__tol': [ 0.0001, 0.0005, 0.001],\n",
    "}\n",
    "\n",
    "\n",
    "adab_scaler_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "adab_scaler_search.fit(features, labels)\n",
    "print(adab_scaler_search.best_params_)\n",
    "\n",
    "test_classifier(adab_scaler_search.best_estimator_, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr'...'SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))]))])\n",
      "\tAccuracy: 0.87644\tPrecision: 0.56111\tRecall: 0.33667\tF1: 0.42083\tF2: 0.36594\n",
      "\tTotal predictions: 4500\tTrue positives:  202\tFalse positives:  158\tFalse negatives:  398\tTrue negatives: 3742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Does using feature scaling INSTEAD of select-from-model using L1 penalty improve scores?\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('clf', adab_grid_search3.best_estimator_)])\n",
    "\n",
    "test_classifier(pipeline, my_dataset, features_list, folds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 3, 'select__k': 15}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('select', SelectKBest(k=15, score_func=<function f_classif at 0x000000000BC02208>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', Pipeline(memory...'SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))]))])\n",
      "\tAccuracy: 0.83933\tPrecision: 0.22422\tRecall: 0.08333\tF1: 0.12151\tF2: 0.09531\n",
      "\tTotal predictions: 4500\tTrue positives:   50\tFalse positives:  173\tFalse negatives:  550\tTrue negatives: 3727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Does using feature scaling along with SelectKBest and PCA instead of L1-based feature selection get better scores?\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('select', SelectKBest()),\n",
    "                     ('pca', PCA(random_state=42)), \n",
    "                     ('clf', adab_grid_search3.best_estimator_)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': [6,10,15,20],\n",
    "    'pca__n_components': [3,5,6],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv=sss, scoring=\"recall\")\n",
    "grid_search.fit(features_scaled, labels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "test_classifier(grid_search.best_estimator_, my_dataset, features_list, folds = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summarizing, the AdaBoost classifier used in conjunction with L1-based feature selection had superior recall and precision scores in comparison with Random Forest (although this was close) and in comparison to other feature selection methods using feature scaling, SelectKBest features, and principal component analysis (PCA).\n",
    "<br><br>\n",
    "The features obtained by the L1-based recursive feature elimination did OK, but not quite as good as all the features originally present with or without the new feature created for having or not having an email address.\n",
    "<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.45, 'select__estimator__tol': 0.0001, 'clf__n_estimators': 18}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('select',\n",
       "   SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "           norm_order=1, prefit=False, threshold=None)),\n",
       "  ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=0.45, n_estimators=18, random_state=42))]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(adab_grid_search3.best_params_)\n",
    "final_clf = adab_grid_search3.best_estimator_\n",
    "final_clf.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('select', SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_...hm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.45, n_estimators=18, random_state=42))])\n",
      "\tAccuracy: 0.87200\tPrecision: 0.52736\tRecall: 0.38550\tF1: 0.44541\tF2: 0.40742\n",
      "\tTotal predictions: 15000\tTrue positives:  771\tFalse positives:  691\tFalse negatives: 1229\tTrue negatives: 12309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(final_clf, my_dataset, features_list)\n",
    "pkl_clf, pkl_dataset, pkl_feature_list = load_classifier_and_data()\n",
    "test_classifier(pkl_clf, pkl_dataset, pkl_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>A look at outliers and top 1 and 5% of financial statistics</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From below, we can see that the top 5% for salaries, bonuses, and total stock value have a much higher percentage of POIs than\n",
    "would be expected based on the baseline POI rate.  The top 1% for these features are even more heavily skewed towards POIs.\n",
    "Loan advances and restricted stock show some signs of high POI skew at the 99 percentile level.\n",
    "\n",
    "Looking at all the values displayed below, we do not see any obviously invalid outlier values.  Since the values that look\n",
    "like outliers have a very high chance of predicting POIs, it would make no sense to exclude them from the analysis given that\n",
    "there is no apriori reason to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_poi_of_top_percent(feature, percentile):\n",
    "    vals = [ data_dict[emp][feature] for emp in data_dict.keys() if data_dict[emp][feature] != 'NaN']\n",
    "    vals = np.array(vals)\n",
    "    print(\"Printing values for feature {}\".format(feature))\n",
    "    print(str(vals))\n",
    "    thresh = np.percentile(vals, percentile)\n",
    "    print(\"{} percentile threshold: {}\".format(percentile, thresh))\n",
    "    poi_for_feature = [ data_dict[emp]['poi'] for emp in data_dict.keys() if data_dict[emp][feature] != 'NaN' and data_dict[emp][feature] > thresh]\n",
    "    print(\"Individuals over threshold are POI: {}\".format(poi_for_feature))\n",
    "    return (sum(map(int, poi_for_feature)) + 0.0)/len(poi_for_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature salary\n",
      "[ 365788  267102  170941       0  243293  267093       0  370448       0\n",
      "  197091  130724  288589  248546  257486       0       0  288542  251654\n",
      "  288558   63744       0  357091  271442       0       0  304110       0\n",
      "  187922       0  213625  249201       0  231330       0  182245       0\n",
      "  211788       0       0       0       0  224305  273746  339288  216582\n",
      "  210500       0       0  272880     477       0  269076  428780  211844\n",
      "       0  206121  174246  510364  365038       0  365163  162779       0\n",
      "  236457       0 1072321  261516  329078       0  184899  192008  263413\n",
      "  262663       0       0  374125  278601       0  199157       0   96840\n",
      "   80818  213999  210692  222093  440698       0  240189  420636  275101\n",
      "       0  314288   94941       0  239502 1111258       0       0       0\n",
      "    6615  655037       0       0  404338       0  259996  317543       0\n",
      "  201955  248146       0       0       0       0       0   76399  262788\n",
      "       0  261809  248017       0  229284  231946  221003  158403       0\n",
      "  250100  492375 1060932  261879  239671       0       0  304588  309946\n",
      "       0   85274  247338  349487  330546       0  415189  265214  278601\n",
      "  274975]\n",
      "95 percentile threshold: 427151.2\n",
      "Individuals over threshold are POI: [0, 0, 1, 1, 1, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('salary', 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature salary\n",
      "[ 365788  267102  170941       0  243293  267093       0  370448       0\n",
      "  197091  130724  288589  248546  257486       0       0  288542  251654\n",
      "  288558   63744       0  357091  271442       0       0  304110       0\n",
      "  187922       0  213625  249201       0  231330       0  182245       0\n",
      "  211788       0       0       0       0  224305  273746  339288  216582\n",
      "  210500       0       0  272880     477       0  269076  428780  211844\n",
      "       0  206121  174246  510364  365038       0  365163  162779       0\n",
      "  236457       0 1072321  261516  329078       0  184899  192008  263413\n",
      "  262663       0       0  374125  278601       0  199157       0   96840\n",
      "   80818  213999  210692  222093  440698       0  240189  420636  275101\n",
      "       0  314288   94941       0  239502 1111258       0       0       0\n",
      "    6615  655037       0       0  404338       0  259996  317543       0\n",
      "  201955  248146       0       0       0       0       0   76399  262788\n",
      "       0  261809  248017       0  229284  231946  221003  158403       0\n",
      "  250100  492375 1060932  261879  239671       0       0  304588  309946\n",
      "       0   85274  247338  349487  330546       0  415189  265214  278601\n",
      "  274975]\n",
      "99 percentile threshold: 1067309.84\n",
      "Individuals over threshold are POI: [1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('salary', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two results we see that 37.5% of those at 95th percentile of salaries are POIs, while less than 20% of the \n",
    "sample are POIs.  Also, 100% of those at the 99th percentile are POIs.  Thus, the \"outliers\" for salary are very likely to be POIs.  <b>Since there is nothing invalid about the values for the salaries and the salaries apparently have some predictive value, there is no reason to exclude the outliers for salary</b>; indeed, this indicates that salary would be a good feature to use in our feature set for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the same results for bonus and total stock value, and not so much for restricted stock and loan advances. \n",
    "There are no values shown below that stand out as being obviously invalid.  Note that values of 'NaN' were excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature bonus\n",
      "[ 600000 1200000  350000       0 1500000  325000       0 2600000       0\n",
      "  400000       0  788750  850000  700000       0       0 1200000 1100000\n",
      "  250000       0       0  850000 3100000       0       0 2000000       0\n",
      "  250000       0 1000000  700000       0  700000       0  200000       0\n",
      " 1700000       0       0       0       0  800000 1000000 8000000       0\n",
      "  425000       0       0  750000       0       0  650000 1500000  200000\n",
      "       0  600000       0 3000000 1100000       0 3000000  100000       0\n",
      "  200000       0 7000000  750000  750000       0  325000  509870  900000\n",
      "  700000       0       0 1150000 1350000       0  350000       0       0\n",
      "       0 5249999  750000       0 1300000       0 1250000 1750000  400000\n",
      "       0  800000       0       0  500000 5600000       0       0       0\n",
      "       0  300000       0       0 1000000       0  325000  450000       0\n",
      " 4175000  600000       0       0       0       0       0  100000 1000000\n",
      "       0  300000  500000       0  400000  850000   70000       0       0\n",
      "  600000  800000 2000000 1000000  400000       0       0 2500000  700000\n",
      "       0       0  300000       0  900000       0 1000000  600000  800000\n",
      "  600000]\n",
      "99 percentile threshold: 6384000.0\n",
      "Individuals over threshold are POI: [0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('bonus', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature bonus\n",
      "[ 600000 1200000  350000       0 1500000  325000       0 2600000       0\n",
      "  400000       0  788750  850000  700000       0       0 1200000 1100000\n",
      "  250000       0       0  850000 3100000       0       0 2000000       0\n",
      "  250000       0 1000000  700000       0  700000       0  200000       0\n",
      " 1700000       0       0       0       0  800000 1000000 8000000       0\n",
      "  425000       0       0  750000       0       0  650000 1500000  200000\n",
      "       0  600000       0 3000000 1100000       0 3000000  100000       0\n",
      "  200000       0 7000000  750000  750000       0  325000  509870  900000\n",
      "  700000       0       0 1150000 1350000       0  350000       0       0\n",
      "       0 5249999  750000       0 1300000       0 1250000 1750000  400000\n",
      "       0  800000       0       0  500000 5600000       0       0       0\n",
      "       0  300000       0       0 1000000       0  325000  450000       0\n",
      " 4175000  600000       0       0       0       0       0  100000 1000000\n",
      "       0  300000  500000       0  400000  850000   70000       0       0\n",
      "  600000  800000 2000000 1000000  400000       0       0 2500000  700000\n",
      "       0       0  300000       0  900000       0 1000000  600000  800000\n",
      "  600000]\n",
      "95 percentile threshold: 2920000.0\n",
      "Individuals over threshold are POI: [0, 0, 0, 1, 1, 1, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('bonus', 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature total_stock_value\n",
      "[  585062 10623258  6678735  1038185  6391065   208510   955873  1662855\n",
      "  7256648   880290  2282768        0   954354   698920  2218275   372205\n",
      "   698242  1416848   725735   384930  1030329  5898997   547143        0\n",
      "   -44093  2072035        0   659249        0  1843816  1918887    98718\n",
      "   126027  2217299  1008941        0   441096   189518   850477   151418\n",
      "   758931   985032   360528  5167144  2493616  2027865        0        0\n",
      "   877611  5243487   371750   987001  3128982  2493616   412878   159211\n",
      "  1034346  6079137  3101279    47304  3614261  1362375   139130  3064208\n",
      "        0 49110078   417619  2606763  1691366   207940   318607   947861\n",
      "   668132   759557  1945360   803094   252055    85641  1621236   221141\n",
      "  7890324  1599641  1110705  1640910  4817796  1794412   343434   126027\n",
      " 22542539   976037        0   495633  7307594        0   511734 26093672\n",
      "        0  1832468  1095040        0    28798        0   368705  6153642\n",
      "  1884748  2056427 15144123  6077885  1729541   494136   431750        0\n",
      "        0   257817  1118394  4221891  8317782        0        0  1014505\n",
      "   192758   597461   896153        0 11884758 30766064  1865087  8831913\n",
      " 14622185 23817930    63014   346663   176378  2332399  1920055        0\n",
      "  5819980  1168042  2070306  3444470        0  2502063  3745048  1080988\n",
      "   778546]\n",
      "95 percentile threshold: 11632458.0\n",
      "Individuals over threshold are POI: [1, 1, 1, 0, 1, 1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('total_stock_value', 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature total_stock_value\n",
      "[  585062 10623258  6678735  1038185  6391065   208510   955873  1662855\n",
      "  7256648   880290  2282768        0   954354   698920  2218275   372205\n",
      "   698242  1416848   725735   384930  1030329  5898997   547143        0\n",
      "   -44093  2072035        0   659249        0  1843816  1918887    98718\n",
      "   126027  2217299  1008941        0   441096   189518   850477   151418\n",
      "   758931   985032   360528  5167144  2493616  2027865        0        0\n",
      "   877611  5243487   371750   987001  3128982  2493616   412878   159211\n",
      "  1034346  6079137  3101279    47304  3614261  1362375   139130  3064208\n",
      "        0 49110078   417619  2606763  1691366   207940   318607   947861\n",
      "   668132   759557  1945360   803094   252055    85641  1621236   221141\n",
      "  7890324  1599641  1110705  1640910  4817796  1794412   343434   126027\n",
      " 22542539   976037        0   495633  7307594        0   511734 26093672\n",
      "        0  1832468  1095040        0    28798        0   368705  6153642\n",
      "  1884748  2056427 15144123  6077885  1729541   494136   431750        0\n",
      "        0   257817  1118394  4221891  8317782        0        0  1014505\n",
      "   192758   597461   896153        0 11884758 30766064  1865087  8831913\n",
      " 14622185 23817930    63014   346663   176378  2332399  1920055        0\n",
      "  5819980  1168042  2070306  3444470        0  2502063  3745048  1080988\n",
      "   778546]\n",
      "99 percentile threshold: 28710211.52\n",
      "Individuals over threshold are POI: [1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('total_stock_value', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature restricted_stock\n",
      "[  585062  3942714  1788391   386335   853064   208510   462384   558801\n",
      "  2046079   409554        0        0   189041   698920        0   153686\n",
      "   698242   360528   540672   384930        0  1552453   466101    32460\n",
      "        0   630137        0   659249        0   378082   283649        0\n",
      "   126027  2217299   407503        0   441096   662086        0   151418\n",
      "    94556   985032   360528  1008149   869220   315068        0        0\n",
      "   441096  1757552        0   379164  1293424   869220        0   141833\n",
      "  1034346  2796177  1478269    47304  1323148        0        0   514847\n",
      "        0 14761694   417619   969729   934065   207940   235370   441096\n",
      "   480632        0   264013   524169   252055    75838   956775   161602\n",
      "   381285        0   157569   189041   365320  1794412        0   126027\n",
      "  2748364   126027        0   378082  2041016        0   511734  6843672\n",
      "        0   405999   208809        0        0        0   463261  4131594\n",
      "   560170   388167 13847074   950730   126027   213063    44093        0\n",
      "        0        0   363428   201483   126027 -2604490   307301   189041\n",
      "        0   196983   259907        0  3576206        0   315068  1787380\n",
      "  4188667  8453763   145796   346663        0  1392142  1248318        0\n",
      "  3654808   576792        0   901657        0  2502063   563798   315068\n",
      "   393818]\n",
      "95 percentile threshold: 3639087.6\n",
      "Individuals over threshold are POI: [0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('restricted_stock', 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature restricted_stock\n",
      "[  585062  3942714  1788391   386335   853064   208510   462384   558801\n",
      "  2046079   409554        0        0   189041   698920        0   153686\n",
      "   698242   360528   540672   384930        0  1552453   466101    32460\n",
      "        0   630137        0   659249        0   378082   283649        0\n",
      "   126027  2217299   407503        0   441096   662086        0   151418\n",
      "    94556   985032   360528  1008149   869220   315068        0        0\n",
      "   441096  1757552        0   379164  1293424   869220        0   141833\n",
      "  1034346  2796177  1478269    47304  1323148        0        0   514847\n",
      "        0 14761694   417619   969729   934065   207940   235370   441096\n",
      "   480632        0   264013   524169   252055    75838   956775   161602\n",
      "   381285        0   157569   189041   365320  1794412        0   126027\n",
      "  2748364   126027        0   378082  2041016        0   511734  6843672\n",
      "        0   405999   208809        0        0        0   463261  4131594\n",
      "   560170   388167 13847074   950730   126027   213063    44093        0\n",
      "        0        0   363428   201483   126027 -2604490   307301   189041\n",
      "        0   196983   259907        0  3576206        0   315068  1787380\n",
      "  4188667  8453763   145796   346663        0  1392142  1248318        0\n",
      "  3654808   576792        0   901657        0  2502063   563798   315068\n",
      "   393818]\n",
      "99 percentile threshold: 11474017.16\n",
      "Individuals over threshold are POI: [1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('restricted_stock', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature loan_advances\n",
      "[       0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0 81525000        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0   400000        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "  2000000        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0]\n",
      "95 percentile threshold: 0.0\n",
      "Individuals over threshold are POI: [1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('loan_advances', 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing values for feature loan_advances\n",
      "[       0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0 81525000        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0   400000        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "  2000000        0        0        0        0        0        0        0\n",
      "        0        0        0        0        0        0        0        0\n",
      "        0]\n",
      "99 percentile threshold: 1296000.0\n",
      "Individuals over threshold are POI: [1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_poi_of_top_percent('loan_advances', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
